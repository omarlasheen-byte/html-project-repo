<!DOCTYPE html>
<html lang="en-US" class=" html_stretched responsive av-preloader-disabled av-default-lightbox  html_header_top html_logo_left html_bottom_nav_header html_menu_left html_large html_header_sticky html_header_shrinking html_header_topbar_active html_mobile_menu_phone html_disabled html_header_searchicon html_content_align_center html_header_unstick_top_disabled html_header_stretch_disabled html_minimal_header html_entry_id_1333 ">
<head>

<meta charset="UTF-8" />

<title>Artificial Intelligence</title>

</head>
<body id="top" class="portfolio-template-default single single-portfolio postid-1333 stretched open_sans cookies-set cookies-accepted give-recurring tribe-no-js" itemscope="itemscope" itemtype="https://schema.org/WebPage">

	<div id='wrap_all'>
<div id='main' data-scroll-offset='116'>
<div class='main_color container_wrap_first container_wrap sidebar_right'><div class='container'><main role="main" itemprop="mainContentOfPage" class='template-page content  av-content-small alpha units'><div class='post-entry post-entry-type-page post-entry-1333'><div class='entry-content-wrapper clearfix'><div class="flex_column av_one_full  flex_column_div av-zero-column-padding first  avia-builder-el-0  el_before_av_textblock  avia-builder-el-first  " style='border-radius:0px; '><div class='avia-image-container  av-styling-   avia-builder-el-1  el_before_av_heading  avia-builder-el-first  avia-align-center ' itemscope="itemscope" itemtype="https://schema.org/ImageObject"><div class='avia-image-container-inner'><img class='avia_image ' src='https://futureoflife.org/wp-content/uploads/2015/11/artificial_intelligence_benefits_risk-1400x430.jpg?x59035' alt='benefits and risks of artificial intelligence' title='artificial_intelligence_benefits_risk' itemprop="contentURL" /></div></div>
<div style='padding-bottom:30px;color:#e06241;' class='av-special-heading av-special-heading-h1 custom-color-heading blockquote modern-quote modern-centered  avia-builder-el-2  el_after_av_image  el_before_av_textblock  '><h1 class='av-special-heading-tag' itemprop="headline"><div class='special-heading-border'><div class='special-heading-inner-border' style='border-color:#e06241'></div></div></div>
</div>
<section class="av_textblock_section" itemscope="itemscope" itemtype="https://schema.org/CreativeWork"><div class='avia_textblock ' itemprop="text"><div style='height:10px' class='hr hr-invisible  avia-builder-el-5  avia-builder-el-no-sibling '><span class='hr-inner '><span class='hr-inner-style'></span></span></div>
</div></section>

<A HREF="pages/secondPage.html">page 2</A>
<A HREF="pages/thirdPage.html">page 3</A>
<A HREF="pages/fifthPage.html">page 4</A>
<A HREF="pages/forthPage.html">page 5</A>

<div style='padding-bottom:10px;' class='av-special-heading av-special-heading-h2  blockquote modern-quote  avia-builder-el-6  el_after_av_textblock  el_before_av_textblock  '><h2 class='av-special-heading-tag' itemprop="headline">What is AI?</h2><div class='special-heading-border'><div class='special-heading-inner-border'></div></div></div>
<section class="av_textblock_section" itemscope="itemscope" itemtype="https://schema.org/CreativeWork"><div class='avia_textblock ' itemprop="text"><p>From SIRI to self-driving cars, artificial intelligence (AI) is progressing rapidly. While science fiction often portrays AI as robots with human-like characteristics, AI can encompass anything from Google&#8217;s search algorithms to IBM&#8217;s Watson to autonomous weapons.</p>
<p><span style="font-weight: 400;">Artificial intelligence today is properly known as narrow AI (or weak AI), in that it is designed to perform a narrow task (e.g. only facial recognition or only internet searches or only driving a car). However, the long-term goal of many researchers is to create general AI (AGI or strong AI). While narrow AI may outperform humans at whatever its specific task is, like playing chess or solving equations, AGI would outperform humans at nearly every cognitive task.</span></p>
</div></section>
<div style='padding-bottom:10px;' class='av-special-heading av-special-heading-h2  blockquote modern-quote  avia-builder-el-8  el_after_av_textblock  el_before_av_textblock  '><h2 class='av-special-heading-tag' itemprop="headline">Why research AI safety?</h2><div class='special-heading-border'><div class='special-heading-inner-border'></div></div></div>
<section class="av_textblock_section" itemscope="itemscope" itemtype="https://schema.org/CreativeWork"><div class='avia_textblock ' itemprop="text"><p>In the near term, the goal of keeping AI&#8217;s impact on society beneficial motivates research in many areas, from economics and law to technical topics such as verification, validity, security and control. Whereas it may be little more than a minor nuisance if your laptop crashes or gets hacked, it becomes all the more important that an AI system does what you want it to do if it controls your car, your airplane, your pacemaker, your automated trading system or your power grid. Another short-term challenge is preventing a devastating arms race in lethal autonomous weapons.</p>
<p>In the long term, an important question is what will happen if the quest for strong AI succeeds and an AI system becomes better than humans at all cognitive tasks. As pointed out by I.J. Good in 1965, designing smarter AI systems is itself a cognitive task. Such a system could potentially undergo recursive self-improvement, triggering an <span style="font-weight: 400;">intelligence explosion leaving human intellect far behind. By inventing revolutionary new technologies, such a superintelligence might help us </span>eradicate war, disease, and poverty, and so the creation of strong AI might be the biggest event in human history. Some experts have expressed concern, though, that it might also be the last, unless we learn to align the goals of the AI with ours before it becomes superintelligent.</p>
<p>There are some who question whether strong AI will ever be achieved, and others who insist that the creation of superintelligent AI is guaranteed to be beneficial. At FLI we recognize both of these possibilities, but also recognize the potential for an artificial intelligence system to intentionally or unintentionally cause great harm. We believe research today will help us better prepare for and prevent such potentially negative consequences in the future, thus enjoying the benefits of AI while avoiding pitfalls.</p>
</div></section>
<div style='padding-bottom:10px;' class='av-special-heading av-special-heading-h2  blockquote modern-quote  avia-builder-el-10  el_after_av_textblock  el_before_av_textblock  '><h2 class='av-special-heading-tag' itemprop="headline">How can AI be dangerous?</h2><div class='special-heading-border'><div class='special-heading-inner-border'></div></div></div>
<section class="av_textblock_section" itemscope="itemscope" itemtype="https://schema.org/CreativeWork"><div class='avia_textblock ' itemprop="text"><p><span style="font-weight: 400;">Most researchers agree that a superintelligent AI is unlikely to exhibit human emotions like love or hate, and that there is no reason to expect AI to become intentionally benevolent or malevolent. </span>Instead, when considering how AI might become a risk, experts think two scenarios most likely:</p>
<ol>
<li style="font-weight: 400;"><span style="font-weight: 400;"><strong>The AI is programmed to do something devastating:</strong> </span>Autonomous weapons are artificial intelligence systems that are programmed to kill. In the hands of the wrong person, these weapons could easily cause mass casualties. Moreover, an AI arms race could inadvertently lead to an AI war that also results in mass casualties. To avoid being thwarted by the enemy, these weapons would be designed to be extremely difficult to simply &#8220;turn off,&#8221; so humans could plausibly lose control of such a situation. This risk is one that&#8217;s present even with narrow AI, but grows as levels of AI intelligence and autonomy increase.</li>
<li style="font-weight: 400;"><span style="font-weight: 400;"><strong>The AI is programmed to do something beneficial, but it develops a destructive method for achieving its goal:</strong> This can happen whenever we fail to fully align the AI&#8217;s goals with ours, which is strikingly difficult. If you ask an obedient intelligent car to take you to the airport as fast as possible, it might get you there chased by helicopters and covered in vomit, doing not what you wanted but literally what you asked for. If a superintelligent system is tasked with a ambitious geoengineering project, it might wreak havoc with our ecosystem as a side effect, and view human attempts to stop it as a threat to be met.</span></li>
</ol>
<p><span style="font-weight: 400;">As these examples illustrate, the concern about advanced AI isn&#8217;t malevolence but competence. </span>A super-intelligent AI will be extremely good at accomplishing its goals, and if those goals aren’t aligned with ours, we have a problem. You’re probably not an evil ant-hater who steps on ants out of malice, but if you’re in charge of a hydroelectric green energy project and there’s an anthill in the region to be flooded, too bad for the ants. A key goal of AI safety research is to never place humanity in the position of those ants.</p>
</div></section>
<div style='padding-bottom:10px;' class='av-special-heading av-special-heading-h2  blockquote modern-quote  avia-builder-el-12  el_after_av_textblock  el_before_av_textblock  '><h2 class='av-special-heading-tag' itemprop="headline">Why the recent interest in AI safety</h2><div class='special-heading-border'><div class='special-heading-inner-border'></div></div></div>
<section class="av_textblock_section" itemscope="itemscope" itemtype="https://schema.org/CreativeWork"><div class='avia_textblock ' itemprop="text"><p><span style="font-weight: 400;">Stephen Hawking, Elon Musk, Steve Wozniak, Bill Gates, and many other big names in science and technology have recently expressed concern in the media and via open letters about the risks posed by AI, joined by many leading AI researchers. Why is the subject suddenly in the headlines? </span></p>
<p><span style="font-weight: 400;">The idea that the quest for strong AI would ultimately succeed was long thought of as science fiction, centuries or more away. However, thanks to recent breakthroughs, many AI milestones, which experts viewed as decades away merely five years ago, have now been reached, making many experts take seriously the possibility of superintelligence in our lifetime. </span><span style="font-weight: 400;">While some experts still guess that human-level AI is centuries away, most AI researches at the 2015 Puerto Rico Conference guessed that it would happen before 2060. Since it may take decades to complete the required safety research, it is prudent to start it now.</span></p>
<p>Because AI has the potential to become more intelligent than any human, we have no surefire way of predicting how it will behave. We can&#8217;t use past technological developments as much of a basis because we&#8217;ve never created anything that has the ability to, wittingly or unwittingly, outsmart us. The best example of what we could face may be our own evolution. People now control the planet, not because we&#8217;re the strongest, fastest or biggest, but because we&#8217;re the smartest. If we&#8217;re no longer the smartest, are we assured to remain in control?</p>
<p>FLI&#8217;s position is that our civilization will flourish as long as we win the race between the growing power of technology and the wisdom with which we manage it. In the case of AI technology, FLI&#8217;s position is that the best way to win that race is not to impede the former, but to accelerate the latter, by supporting AI safety research.</p>
</div></section>






</body>
</html>
